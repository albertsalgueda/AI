# -*- coding: utf-8 -*-
"""MAB - baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wkZGtb9vQUGF5NBy2C4GvRYyJtnxWH_G
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

class Env(object):
  def __init__(self,reward_probabilities,actual_rewards):
    if len(reward_probabilities) != len(actual_rewards):
      raise Exception(f"the size of the rewards probabilities does not match size of actual rewards")
    self.reward_probabilities = reward_probabilities
    self.actual_rewards = actual_rewards
    self.k_arms = len(reward_probabilities)
  
  def choose_arm(self,arm):
    if arm < 0 or arm >= self.k_arms:
      raise Exception(f'arm must be between 0 and {self.k_arms}')
    #each arm has a setting probability of paying a rewards ( in simulation we generate a random number )
    return self.actual_rewards[arm] if np.random.random() < self.reward_probabilities[arm] else 0.0

env = Env(reward_probabilities=[0.62,0.05,0.87,0.49],actual_rewards=[1.0,1.0,1.0,1.0])

print(env.choose_arm(2))

[env.choose_arm(0) for _ in range(10)]

"""# Calculate average through incremental sampling

calculate an average without summing all the elements of the list ( memory-efficient approach ) 

```
new_estimate = old_estimate + (1/n)*(reward-old_estimate) 
```
or more genarally,

```
new_estimate = old_estimate + step_size*(reward-old_estimate) 
```

# Incremental Sampling For Non-stationary Bandit Problems


```
new_estimage = old_estimate + (1/n) * (reward - old_estimate)
```


In the incremental sampling formular above, the term (1/n) gets smaller as n gets larger. This means that, rewards from later time steps contribute little to the new estimate of the average and this makes it unfit for bandit problems with non-stationary reward distributions. To curb this, another form of this formula with a fixed step size should be used. The step size must be a number between 0 and 1. This way, rewards from later time steps contribute more to the estimation of the average and this makes it fit for non-stationary bandit problems. The following general update rule is very common throughout Reinforcement Learning especially in Temporal Difference learning algorithms.
```
new_estimage = old_estimate + (step_size) * (reward - old_estimate)
```
The closer step_size is to 1, the more new rewards contribute to the estimate. 
Let's verify this in code.
"""

time_steps = np.arange(1,50)
weights = 1/time_steps

plt.plot(time_steps, weights)

"""# Implementing a Random-behaving agent"""

class RandomAgent(object):

  def __init__(self, env, max_iterations=500):
    self.env = env
    self.iterations = max_iterations

    self.q_values = np.zeros(self.env.k_arms)
    self.arm_counts = np.zeros(self.env.k_arms)
    self.arm_rewards = np.zeros(self.env.k_arms)

    self.rewards = [0.0]
    self.cum_rewards = [0.0]

  def act(self):
    for i in range(self.iterations):
      arm = np.random.choice(self.env.k_arms)
      reward = self.env.choose_arm(arm)

      self.arm_counts[arm] = self.arm_counts[arm] + 1
      self.arm_rewards[arm] = self.arm_rewards[arm] + reward

      self.q_values[arm] = self.q_values[arm] + (1/self.arm_counts[arm]) * (reward - self.q_values[arm])
      self.rewards.append(reward)
      self.cum_rewards.append(sum(self.rewards) / len(self.rewards))

    return {"arm_counts": self.arm_counts, "rewards": self.rewards, "cum_rewards": self.cum_rewards}

test_env = Env(reward_probabilities=[0.62,0.05,0.87,0.49],actual_rewards=[1.0,1.0,1.0,1.0])
random_agent = RandomAgent(test_env)
random_agent_result = random_agent.act()

total_rewards = sum(random_agent_result['rewards'])
print("total reward:", total_rewards)

cum_rewards = random_agent_result['cum_rewards']
arm_counts = random_agent_result['arm_counts']

fig = plt.figure(figsize=[30,10])

ax1 = fig.add_subplot(121)
#maximum posible rewards
ax1.plot([.87 for _ in range(random_agent.iterations)])
#actual rewards
ax1.plot(cum_rewards, label="cumulative rewards")
ax1.set_xlabel("Time steps")
ax1.set_ylabel("Cumulative rewards")

#let's check if the agent is intelligent 
ax2 = fig.add_subplot(122) 

ax2.bar([i for i in range(len(arm_counts))],arm_counts)

print(f'Environment Action Values: {env.reward_probabilities}')
print(f'Random Agent Action Values: {random_agent.q_values}')